---
title: "Lecture 12: Bayesian Model Fitting"
output:
  revealjs::revealjs_presentation:
    theme: white
    center: true
    transition: none
    incremental: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(message = FALSE)
library(spBayes)
library(geoR)
library(dplyr)
library(ggplot2)
library(mnormt)
library(rjags)
```

# Class Intro

## Intro Questions 
- MCMC methods take samples from the joint posterior distribution for the parameters, say $\boldsymbol{\theta} = \{\beta, \sigma^2, \tau^2, \phi \}$. Discuss how this relates to integration *and* how the collection of samples of $\boldsymbol{\theta}$ can be used to make predictions at other spatial locations.

- For Today:
    - Fitting Bayesian Models

# Likelihood Based Model Fitting

## Variogram Based Model Fitting
- Up until now, we have used a least-squares approach with the variogram to estimate the covariance parameters.

- How would you do this if there were covariates that could be used to explain the process?

- <small> Use the residuals from a linear model. </small>

## Software Demos

- We are going to look at three options for fitting Bayesian spatial models.

1. `krige.bayes()` in `geoR`,
2. `spLM()` in `spBayes`, and
3. using JAGS.

- All of the frameworks are acceptable, but have different strengths and weaknesses.


## JAGS

- JAGS (Just Another Gibbs Sampler) can be called from R using the following framework:
1. Definition of the model (in BUGS)
2. Compile Model
3. Draw Samples

- [JAGS manual](http://web.sgh.waw.pl/~atoroj/ekonometria_bayesowska/jags_user_manual.pdf)

## JAGS demo - Basic Regression
- First we will use JAGS to fit a linear regression model. Use the code in the next slide to do this and answer the following questions.
1. What is the sampling model in this case? Note that dnorm takes the precision (= 1 / variance) as an argument.
2. What priors are specified in this model?
3. How do the results match your expectation and that of `lm()`?


## Code

```{r}
# Simulate Data
set.seed(02142019)
alpha.true <- 0
beta.true <- 1
sigma.true <- 2
num.pts <- 100

x <- runif(num.pts,0,10)
y <- rnorm(num.pts, mean = rep(alpha.true, num.pts) + x*beta.true, sd = sigma.true)
data.frame(x=x, y=y) %>% ggplot(aes(x=x,y=y)) + geom_point() + geom_smooth(method='lm')

# Specify data for JAGS           
data.in <- list(x = x, y = y, N = num.pts)

#Define Model
model_string <- "model{
  # Likelihood
  for(i in 1:N){
    y[i]   ~ dnorm(mu[i], sigmasq.inv)
    mu[i] <- alpha + beta * x[i]
  }

  # Priors
  sigma <- 1 / sqrt(sigmasq.inv)
  alpha ~ dnorm(0, 1.0E-6)
  beta ~ dnorm(0, 1.0E-6)
  sigmasq.inv ~ dgamma(1E-6, 1E-6)
}"

# compile model
model <- jags.model(textConnection(model_string), data = data.in)

# burn in
update(model, 10000)

# draw samples
samp <- coda.samples(model, 
        variable.names=c("alpha","beta","sigma"), 
        n.iter=20000)

# plot samples
summary(samp)
plot(samp)

# compare with lm
summary(lm(y~x))
```


## JAGS demo - Spatial Regression
- Next we consider a more complicated regression model using JAGS, that with the exponential covariance function.

1. What is `data.spatial` and how is it used in JAGS?
2. What priors are used in this case? Do they seem reasonable?
3. How do your results compare with your expectation?

## Code 
```{r, eval = F}
# simulate data
set.seed(02142019)
num.pts <- 75
sigmasq.true <- 3
tausq.true <- .50
phi.true <- 2
x1 <- runif(num.pts, max = 10)
x2 <- runif(num.pts, max = 10)
d <- dist(cbind(x1,x2), upper=T, diag = T) %>% as.matrix()
Omega <- sigmasq.true * exp(-d * phi.true) + tausq.true * diag(num.pts)
alpha.true <- 0
beta.true <- 1
x.reg <- rnorm(num.pts)
y = rmnorm(1, x.reg*beta.true, Omega)
GP.dat <- data.frame(x1 = x1, x2 = x2, y = y)
GP.dat %>% ggplot(aes(x=x1, y = x2, z=y)) + geom_point(aes(color=y)) +   scale_colour_gradient2() + theme_dark()


# Specify data for JAGS           
data.spatial <- list(x.reg = x.reg, y = y, N = num.pts, d = d)

#Define Model
model.spatial <- "model{
# data | process
  for(i in 1:N){
    y[i]   ~ dnorm(mu[i], tausq.inv)
    mu[i] <- alpha + beta * x.reg[i] + W[i]
    muW[i] <- 0
  }

# process | parameters
  W[1:N] ~ dmnorm(muW[], Omega[,])
  
# parameters  
  alpha ~ dnorm(0, .0001)
  beta ~ dnorm(0, .0001)
  tausq.inv ~ dgamma(.001, .001)
  tausq <- 1 / tausq.inv
  sigmasq.inv ~ dgamma(.001, .001)
  sigmasq <- 1 / sigmasq.inv
  phi ~ dunif(1 , 3)
  
# build omega
  for (i in 1:N){
    for (j in 1:N){
      H[i,j] <- (1/sigmasq.inv) * exp(-phi *d[i,j])
    }
  }
  Omega[1:N,1:N] <- inverse(H[1:N,1:N])

}"

# compile model
model <- jags.model(textConnection(model.spatial), data = data.spatial)

# burn in
update(model, 10000)

# draw samples
samp <- coda.samples(model, 
        variable.names=c("alpha","beta","phi", "tausq",'sigmasq'), 
        n.iter=5000)

# plot samples
summary(samp)
plot(samp)
```



## `krige.bayes()` demo
- For this demonstration we will explore the `krige.bayes()` function in R using a modified script from the function description. With this exploration, answer the following questions.
1. What does the `grf()` function do?
2. Explain the parameters in the `prior.control()` section.
3. Describe the output from `hist(ex.bayes)`.
4. What are the four figures generated from the `image()` function?


## Code

```{r, eval = F, echo=T}
set.seed(02132019)
# generating a simulated data-set
ex.data <- grf(100, cov.pars=c(10, .15), cov.model="exponential", nugget = 1)
#
data.frame(x1 = ex.data$coords[,'x'], x2 = ex.data$coords[,'y'], y = ex.data$data) %>% ggplot(aes(x = x1, y = x2)) + geom_point(aes(color = y)) + scale_color_gradient2() + theme_dark()


# defining the grid of prediction locations:
ex.grid <- as.matrix(expand.grid(seq(0,1,l=15), seq(0,1,l=15)))
#
# computing posterior and predictive distributions
# (warning: the next command can be time demanding)
ex.bayes <- krige.bayes(ex.data, loc=ex.grid,
                        model = model.control(cov.m="exponential"),
                        prior = prior.control(beta.prior = 'flat',
                                              sigmasq.prior = 'reciprocal',
                                              phi.discrete=seq(0, 0.7, l=25),
                                              phi.prior="uniform", 
                                              tausq.rel.discrete = seq(0, 1, l=25),
                                              tausq.rel.prior = 'uniform'))

# Plot histograms with samples from the posterior
par(mfrow=c(4,1))
hist(ex.bayes)
par(mfrow=c(1,1))

# Plotting empirical variograms and some Bayesian estimates:
plot(variog(ex.data, max.dist=1), ylim=c(0, 25))
# and adding lines with median and quantiles estimates
my.summary <- function(x){quantile(x, prob = c(0.05, 0.5, 0.95))}
lines(ex.bayes, summ = my.summary, ty="l", lty=c(2,1,2), col=1)

# Plotting some prediction results
op <- par(no.readonly = TRUE)
par(mfrow=c(2,2), mar=c(4,4,2.5,0.5), mgp = c(2,1,0))
image(ex.bayes, val = 'mean', main="predicted values")
image(ex.bayes, val="variance", main="prediction variance")
image(ex.bayes, val= "simulation", number.col=1,
      main="a simulation from the \npredictive distribution")
image(ex.bayes, val= "simulation", number.col=2,
      main="another simulation from \nthe predictive distribution")
#
par(op)
```

## `spLM()` demo
- Another option for fitting Bayesian spatial models is the `spLM()` function in the `spBayes` package. Using the code on the next slide, answer the following questions.

1. What is `w`?
2. What does the `tuning` argument in `spLM()` control?
3. What does the following code return `summary(m.1$p.beta.recover.samples)$quantiles`?
4. Describe the final figure generated by this code.


## Code

```{r, eval = F, echo = T}
set.seed(02142019)
rmvn <- function(n, mu=0, V = matrix(1)){
  p <- length(mu)
  if(any(is.na(match(dim(V),p))))
    stop("Dimension problem!")
  D <- chol(V)
  t(matrix(rnorm(n*p), ncol=p)%*%D + rep(mu,rep(n,p)))
}

n <- 100
coords <- cbind(runif(n,0,1), runif(n,0,1))
X <- as.matrix(cbind(1, rnorm(n)))

B <- as.matrix(c(1,5))
p <- length(B)

sigma.sq <- 2
tau.sq <- 0.1
phi <- 3/0.5

D <- as.matrix(dist(coords))
R <- exp(-phi*D)
w <- rmvn(1, rep(0,n), sigma.sq*R)
y <- rnorm(n, X%*%B + w, sqrt(tau.sq))

data.frame(x1 = coords[,1], x2 = coords[,2], y = y) %>% ggplot(aes(x = x1, y = x2)) + geom_point(aes(color = y)) + scale_color_gradient2() + theme_dark() 

n.samples <- 5000

starting <- list("phi"=3/0.5, "sigma.sq"=50, "tau.sq"=1)

tuning <- list("phi"=.1, "sigma.sq"=.1, "tau.sq"=.1)

priors <- list("beta.Norm"=list(rep(0,p), diag(1000,p)),
                 "phi.Unif"=c(3/1, 3/0.1), "sigma.sq.IG"=c(2, 2),
                 "tau.sq.IG"=c(2, 0.1))

m.1 <- spLM(y~X-1, coords=coords, starting=starting,
            tuning=tuning, priors=priors, cov.model="exponential",
            n.samples=n.samples, verbose=TRUE, n.report=500)

burn.in <- 0.5*n.samples

##recover beta and spatial random effects
m.1 <- spRecover(m.1, start=burn.in, verbose=FALSE)

summary(m.1$p.theta.recover.samples)$quantiles

summary(m.1$p.beta.recover.samples)$quantiles

m.1.w.summary <- summary(mcmc(t(m.1$p.w.recover.samples)))$quantiles[,c(3,1,5)]

plot(w, m.1.w.summary[,1], xlab="Observed w", ylab="Fitted w",
     xlim=range(w), ylim=range(m.1.w.summary), main="Spatial random effects")
arrows(w, m.1.w.summary[,1], w, m.1.w.summary[,2], length=0.02, angle=90)
arrows(w, m.1.w.summary[,1], w, m.1.w.summary[,3], length=0.02, angle=90)
lines(range(w), range(w))
```




## Other Modeling Options
- JAGS is another option for fitting general Bayesian models.
- Additionally, these models can also be implemented from scratch.
